{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "ML_09_Ensemble_LAB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyworld19/ds-school-advanced/blob/master/ML_09_Ensemble_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89ijUI58NDeg"
      },
      "source": [
        "### Ensemble"
      ],
      "id": "89ijUI58NDeg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFB8SAohNDej"
      },
      "source": [
        "#### 위스콘신 유방암 진단(이진 분류)"
      ],
      "id": "OFB8SAohNDej"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsacEzzmNDej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bddba75a-5b70-42d0-f7f7-055e5527c223"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "# 암 데이터를 가져와 cancer로 이름을 지정한다\n",
        "cancer = load_breast_cancer()\n",
        "# 'cancer.csv' 로 저장한다\n",
        "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
        "df['class'] = cancer.target\n",
        "df.to_csv('cancer.csv', index=False)\n",
        "\n",
        "target_names = cancer.target_names\n",
        "print(target_names)"
      ],
      "id": "OsacEzzmNDej",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['malignant' 'benign']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMKmFFtzNDek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97514e63-c566-495f-f4f4-7e77d8ba9295"
      },
      "source": [
        "import pandas as pd\n",
        "# 암 데이터를 가져와 cancer로 이름을 지정한다\n",
        "cancer = pd.read_csv('cancer.csv')\n",
        "X = cancer.iloc[:, :-1]\n",
        "y = cancer.iloc[:, -1]\n",
        "X.shape, y.shape"
      ],
      "id": "EMKmFFtzNDek",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((569, 30), (569,))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47fQHGd1NDel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707f8b06-fea5-4ac1-9ee7-6950803e44a3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled,\n",
        "                                                    y,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=0)\n",
        "[a.shape for a in (x_train, x_test, y_train, y_test)]"
      ],
      "id": "47fQHGd1NDel",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(426, 30), (143, 30), (426,), (143,)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp-oNeoBNDem"
      },
      "source": [
        "#### voting 참여 분류기 생성\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
      ],
      "id": "Yp-oNeoBNDem"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c8rZHMsNDem",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb47c7f-d869-4c0f-ac2f-2def40503aba"
      },
      "source": [
        "# VotingClassifier의 사용법 확인\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "help(VotingClassifier)"
      ],
      "id": "4c8rZHMsNDem",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class VotingClassifier in module sklearn.ensemble._voting:\n",
            "\n",
            "class VotingClassifier(sklearn.base.ClassifierMixin, _BaseVoting)\n",
            " |  VotingClassifier(estimators, voting='hard', weights=None, n_jobs=None, flatten_transform=True)\n",
            " |  \n",
            " |  Soft Voting/Majority Rule classifier for unfitted estimators.\n",
            " |  \n",
            " |  .. versionadded:: 0.17\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <voting_classifier>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  estimators : list of (str, estimator) tuples\n",
            " |      Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n",
            " |      of those original estimators that will be stored in the class attribute\n",
            " |      ``self.estimators_``. An estimator can be set to ``'drop'``\n",
            " |      using ``set_params``.\n",
            " |  \n",
            " |      .. deprecated:: 0.22\n",
            " |         Using ``None`` to drop an estimator is deprecated in 0.22 and\n",
            " |         support will be dropped in 0.24. Use the string ``'drop'`` instead.\n",
            " |  \n",
            " |  voting : str, {'hard', 'soft'} (default='hard')\n",
            " |      If 'hard', uses predicted class labels for majority rule voting.\n",
            " |      Else if 'soft', predicts the class label based on the argmax of\n",
            " |      the sums of the predicted probabilities, which is recommended for\n",
            " |      an ensemble of well-calibrated classifiers.\n",
            " |  \n",
            " |  weights : array-like, shape (n_classifiers,), optional (default=`None`)\n",
            " |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
            " |      predicted class labels (`hard` voting) or class probabilities\n",
            " |      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
            " |  \n",
            " |  n_jobs : int or None, optional (default=None)\n",
            " |      The number of jobs to run in parallel for ``fit``.\n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |  \n",
            " |  flatten_transform : bool, optional (default=True)\n",
            " |      Affects shape of transform output only when voting='soft'\n",
            " |      If voting='soft' and flatten_transform=True, transform method returns\n",
            " |      matrix with shape (n_samples, n_classifiers * n_classes). If\n",
            " |      flatten_transform=False, it returns\n",
            " |      (n_classifiers, n_samples, n_classes).\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  estimators_ : list of classifiers\n",
            " |      The collection of fitted sub-estimators as defined in ``estimators``\n",
            " |      that are not 'drop'.\n",
            " |  \n",
            " |  named_estimators_ : Bunch object, a dictionary with attribute access\n",
            " |      Attribute to access any fitted sub-estimators by name.\n",
            " |  \n",
            " |      .. versionadded:: 0.20\n",
            " |  \n",
            " |  classes_ : array-like, shape (n_predictions,)\n",
            " |      The classes labels.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  VotingRegressor: Prediction voting regressor.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> from sklearn.linear_model import LogisticRegression\n",
            " |  >>> from sklearn.naive_bayes import GaussianNB\n",
            " |  >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
            " |  >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
            " |  >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
            " |  >>> clf3 = GaussianNB()\n",
            " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
            " |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
            " |  >>> eclf1 = VotingClassifier(estimators=[\n",
            " |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
            " |  >>> eclf1 = eclf1.fit(X, y)\n",
            " |  >>> print(eclf1.predict(X))\n",
            " |  [1 1 1 2 2 2]\n",
            " |  >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n",
            " |  ...                eclf1.named_estimators_['lr'].predict(X))\n",
            " |  True\n",
            " |  >>> eclf2 = VotingClassifier(estimators=[\n",
            " |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
            " |  ...         voting='soft')\n",
            " |  >>> eclf2 = eclf2.fit(X, y)\n",
            " |  >>> print(eclf2.predict(X))\n",
            " |  [1 1 1 2 2 2]\n",
            " |  >>> eclf3 = VotingClassifier(estimators=[\n",
            " |  ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
            " |  ...        voting='soft', weights=[2,1,1],\n",
            " |  ...        flatten_transform=True)\n",
            " |  >>> eclf3 = eclf3.fit(X, y)\n",
            " |  >>> print(eclf3.predict(X))\n",
            " |  [1 1 1 2 2 2]\n",
            " |  >>> print(eclf3.transform(X).shape)\n",
            " |  (6, 6)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      VotingClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      _BaseVoting\n",
            " |      sklearn.base.TransformerMixin\n",
            " |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
            " |      sklearn.base.MetaEstimatorMixin\n",
            " |      sklearn.utils.metaestimators._BaseComposition\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, estimators, voting='hard', weights=None, n_jobs=None, flatten_transform=True)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit the estimators.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples and\n",
            " |          n_features is the number of features.\n",
            " |      \n",
            " |      y : array-like, shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,) or None\n",
            " |          Sample weights. If None, then samples are equally weighted.\n",
            " |          Note that this is supported only if all underlying estimators\n",
            " |          support sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict class labels for X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      maj : array-like, shape (n_samples,)\n",
            " |          Predicted class labels.\n",
            " |  \n",
            " |  transform(self, X)\n",
            " |      Return class labels or probabilities for X for each estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples and\n",
            " |          n_features is the number of features.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      probabilities_or_labels\n",
            " |          If `voting='soft'` and `flatten_transform=True`:\n",
            " |              returns array-like of shape (n_classifiers, n_samples *\n",
            " |              n_classes), being class probabilities calculated by each\n",
            " |              classifier.\n",
            " |          If `voting='soft' and `flatten_transform=False`:\n",
            " |              array-like of shape (n_classifiers, n_samples, n_classes)\n",
            " |          If `voting='hard'`:\n",
            " |              array-like of shape (n_samples, n_classifiers), being\n",
            " |              class labels predicted by each classifier.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  predict_proba\n",
            " |      Compute probabilities of possible outcomes for samples in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      avg : array-like, shape (n_samples, n_classes)\n",
            " |          Weighted average probability for each class per sample.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.TransformerMixin:\n",
            " |  \n",
            " |  fit_transform(self, X, y=None, **fit_params)\n",
            " |      Fit to data, then transform it.\n",
            " |      \n",
            " |      Fits transformer to X and y with optional parameters fit_params\n",
            " |      and returns a transformed version of X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : numpy array of shape [n_samples, n_features]\n",
            " |          Training set.\n",
            " |      \n",
            " |      y : numpy array of shape [n_samples]\n",
            " |          Target values.\n",
            " |      \n",
            " |      **fit_params : dict\n",
            " |          Additional fit parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
            " |          Transformed array.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get the parameters of an estimator from the ensemble.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool\n",
            " |          Setting it to True gets the various classifiers and the parameters\n",
            " |          of the classifiers as well.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of an estimator from the ensemble.\n",
            " |      \n",
            " |      Valid parameter keys can be listed with `get_params()`.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : keyword arguments\n",
            " |          Specific parameters using e.g.\n",
            " |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
            " |          parameters of the stacking estimator, the individual estimator of\n",
            " |          the stacking estimators can also be set, or can be removed by\n",
            " |          setting them to 'drop'.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
            " |  \n",
            " |  named_estimators\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Frn9IKNDen"
      },
      "source": [
        "# VotingRegressor의 사용법 확인\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "VotingRegressor?"
      ],
      "id": "d3Frn9IKNDen",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXBiW4iNDen"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# n_neighbors 를 각각 3, 5로 갖는 KNN 모델 두 개 만들기\n",
        "# max_iter=10000 인 LogisticRegression 모델 만들기\n",
        "# max_depth 를 각각 3, 5로 갖는 DecisionTree 모델 두 개 만들기\n",
        "knn1 = KNeighborsClassifier(3)\n",
        "knn2 = KNeighborsClassifier(5)\n",
        "lr = LogisticRegression(max_iter=10000)\n",
        "dt3 = DecisionTreeClassifier(max_depth=3)\n",
        "dt5 = DecisionTreeClassifier(max_depth=5)"
      ],
      "id": "PmXBiW4iNDen",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfdlfrDVNDeo"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "names = ['knn1', 'knn2', 'lr', 'dt3', 'dt5']\n",
        "estimator = [knn1, knn2, lr, dt3, dt5]\n",
        "# estimators = [('knn1', knn1), ('knn2', knn2), ...]  와 같이 생성\n",
        "estimators = list(zip(names, estimator))\n",
        "\n",
        "#Hard Voting\n",
        "hard = VotingClassifier(estimators, voting='hard')\n",
        "\n",
        "#Soft Voting\n",
        "soft = VotingClassifier(estimators, voting='soft')"
      ],
      "id": "GfdlfrDVNDeo",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEO5BIheNDeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f29ea2-dc25-4d4f-924c-44b09c567d03"
      },
      "source": [
        "names = ['hard', 'soft', 'knn1', 'knn2', 'lr', 'dt3', 'dt5']\n",
        "estimators = [hard, soft, knn1, knn2, lr, dt3, dt5]\n",
        "\n",
        "# 반복문을 사용한 ensemble 및 단독 모델 성능 비교\n",
        "for name, model in zip(names, estimators):\n",
        "    model.fit(x_train, y_train)\n",
        "    print(f'{name:4s} Train Accuracy {model.score(x_train, y_train)*100 : .2f}%')\n",
        "    print(f'{name:4s} Test  Accuracy {model.score(x_test, y_test)*100 : .2f}%')"
      ],
      "id": "eEO5BIheNDeo",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hard Train Accuracy  99.30%\n",
            "hard Test  Accuracy  96.50%\n",
            "soft Train Accuracy  99.77%\n",
            "soft Test  Accuracy  94.41%\n",
            "knn1 Train Accuracy  98.83%\n",
            "knn1 Test  Accuracy  95.10%\n",
            "knn2 Train Accuracy  98.36%\n",
            "knn2 Test  Accuracy  95.10%\n",
            "lr   Train Accuracy  99.06%\n",
            "lr   Test  Accuracy  95.80%\n",
            "dt3  Train Accuracy  97.65%\n",
            "dt3  Test  Accuracy  91.61%\n",
            "dt5  Train Accuracy  100.00%\n",
            "dt5  Test  Accuracy  90.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI5e56JQNDep"
      },
      "source": [
        "### 배깅(Bagging) 방식 - RandomForest\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ],
      "id": "XI5e56JQNDep"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt9OK-7bNDep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbaf9797-a5f8-4779-96d9-0d9216837ce0"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# train 데이터를 사용하여 RandomForestClassifier를 학습한다\n",
        "# max_deptn=5, random_state=0 사용\n",
        "# train, test 성능을 확인한다\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0).fit(x_train, y_train)\n",
        "model.score(x_train, y_train), model.score(x_test, y_test)"
      ],
      "id": "jt9OK-7bNDep",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9882629107981221, 0.9440559440559441)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To1V8NqzNDep"
      },
      "source": [
        "### 부스팅(Boosting) 방식 : GradientBoosting"
      ],
      "id": "To1V8NqzNDep"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_niIJmTWNDeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66368ad3-b73f-4dff-9e59-87bf4dd88951"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# train 데이터를 사용하여 GradientBoostingClassifier를 학습한다\n",
        "# train, test 성능을 확인한다\n",
        "model = GradientBoostingClassifier(random_state=0).fit(x_train, y_train)\n",
        "model.score(x_train, y_train), model.score(x_test, y_test)"
      ],
      "id": "_niIJmTWNDeq",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.958041958041958)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB6yyP2JNDeq"
      },
      "source": [
        "### Stacking 방식 : StackingClassifier"
      ],
      "id": "xB6yyP2JNDeq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCCsevPHNDeq"
      },
      "source": [
        "StackingClassifier?"
      ],
      "id": "hCCsevPHNDeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0cctmtENDeq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92453331-c866-4a40-ee58-35c728935aab"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# RandomForestClassifier, GradientBoostingClassifier 를 estimators로 사용하고\n",
        "# LogisticRegression을 final_estimator로 사용한다\n",
        "# 이때, estimator 각각에 이름은 'rf', 'gb'로 사용한다\n",
        "name = ['rf', 'gb']\n",
        "estimator = [RandomForestClassifier(), GradientBoostingClassifier()]\n",
        "estimators = list(zip(name, estimator))\n",
        "final_estimator = LogisticRegression(max_iter=10000, random_state=0)\n",
        "model = StackingClassifier(estimators, final_estimator).fit(x_train, y_train)\n",
        "model.score(x_train, y_train), model.score(x_test, y_test)"
      ],
      "id": "j0cctmtENDeq",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.958041958041958)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFd8ICNmNDer"
      },
      "source": [
        "### XGBOOST(Extream Gradient Boost)"
      ],
      "id": "CFd8ICNmNDer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePpLWYWMNDer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17be6fa-8b57-45fd-d22d-f0ad21f9b2c7"
      },
      "source": [
        "!pip install xgboost"
      ],
      "id": "ePpLWYWMNDer",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok76LmekNDer"
      },
      "source": [
        "#### XGBOOST 분류 실습\n",
        "- https://xgboost.readthedocs.io/en/latest/index.html\n",
        "- https://xgboost.readthedocs.io/en/latest/python/python_api.html"
      ],
      "id": "ok76LmekNDer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBKtv2BeNDes"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier"
      ],
      "id": "DBKtv2BeNDes",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8g_3vb_NDes"
      },
      "source": [
        "XGBClassifier?"
      ],
      "id": "G8g_3vb_NDes",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MD66dIxd1NM"
      },
      "source": [
        "# 1. 유방암 데이터를 불러온다\n",
        "# 2. random_state=0 으로 사용하여 데이터를 75%:25% 로 분리한다\n",
        "cancer = load_breast_cancer()\n",
        "x_train, x_test, y_train, y_test = train_test_split(cancer.data, \n",
        "                                                    cancer.target, \n",
        "                                                    random_state=0, \n",
        "                                                    #stratify=cancer.target -> 안쓰니까 성능이 더 좋게 나옴\n",
        "                                                    ) "
      ],
      "id": "-MD66dIxd1NM",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8T4qXnLNDes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e77900e-e983-4441-b942-d5253357d3c7"
      },
      "source": [
        "# 3. XGBClassifier 를 사용하여 model을 만들고 학습시킨 뒤 성능을 확인한다\n",
        "#   n_estimators=400, learning_rate=0.1, max_depth=3, \n",
        "#   use_label_encoder=False (Deprecated) eval_metric='mlogloss' 을 사용한다\n",
        "model = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='mlogloss')\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_train, y_train), model.score(x_test, y_test)"
      ],
      "id": "v8T4qXnLNDes",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.986013986013986)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFKnUQ1XNDes"
      },
      "source": [
        "#### XGBOOST 회귀 실습"
      ],
      "id": "CFKnUQ1XNDes"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d77qRuc6NDes"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "boston = load_boston()\n",
        "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "df['PRICE'] = boston.target\n",
        "df.to_csv('boston.csv', index=False)"
      ],
      "id": "d77qRuc6NDes",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3oRCcLeNDet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b45dfde-1407-4e86-8a0c-43246e5db8ba"
      },
      "source": [
        "import pandas as pd\n",
        "boston = pd.read_csv('boston.csv')\n",
        "X = boston.iloc[:, :-1]\n",
        "y = boston.iloc[:, -1]\n",
        "X.shape, y.shape"
      ],
      "id": "-3oRCcLeNDet",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((506, 13), (506,))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7sKAfM8NDet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf96030-1731-4aac-a9ac-066a376971aa"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# random_state=0 으로 사용하여 데이터를 75%:25% 로 분리한다\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# XGBRegressor 를 사용하여 model을 만들고 학습시킨 뒤 성능을 확인한다\n",
        "#   다음 값을 사용한다\n",
        "#   n_estimators=1000, learning_rate=0.2, max_depth=3, objective ='reg:squarederror'\n",
        "model = xgb.XGBRegressor(n_estimators=1000, \n",
        "                         learning_rate=0.2, \n",
        "                         max_depth=3, \n",
        "                         objective ='reg:squarederror')\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_train, y_train), model.score(x_test, y_test)"
      ],
      "id": "I7sKAfM8NDet",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9999998554210379, 0.7788751773436555)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxTDjlFfNDet"
      },
      "source": [
        "### lightGBM"
      ],
      "id": "MxTDjlFfNDet"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGx7huKMNDet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563d4abb-7877-4a40-f460-213df07e9ef8"
      },
      "source": [
        "!pip install lightgbm"
      ],
      "id": "SGx7huKMNDet",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaBhSsgNDeu"
      },
      "source": [
        "### ligthGBM  분류\n",
        "- https://lightgbm.readthedocs.io/en/latest/\n",
        "- https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier"
      ],
      "id": "ZwaBhSsgNDeu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwYYabiVNDeu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "# 암 데이터를 가져와 분리하여 데이터 준비\n",
        "cancer =pd.read_csv('cancer.csv')\n",
        "X = cancer.iloc[:, :-1]\n",
        "y = cancer.iloc[:, -1]\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    stratify=y,\n",
        "                                                    random_state=0)"
      ],
      "id": "pwYYabiVNDeu",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVTCAtzfNDeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa60d1a-ea16-46dc-c9b2-660ab9584626"
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "# LGBMClassifier를 사용하여 학습 후 성능 출력\n",
        "\n",
        "# n_estimators=100으로 사용한다 (여러 가지로 변경해 볼 것)\n",
        "model = LGBMClassifier(n_estimators=400).fit(x_train, y_train)\n",
        "print('train : {:.3f}, test : {:.3f}'.format(model.score(x_train, y_train),\n",
        "                                             model.score(x_test, y_test)))"
      ],
      "id": "XVTCAtzfNDeu",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 1.000, test : 0.951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyVDrANFNDeu"
      },
      "source": [
        "# XGBClassifier 를 사용하여 model을 만들고 학습시킨 뒤 성능을 확인한다\n",
        "#   n_estimators=400, learning_rate=0.1, max_depth=3, \n",
        "#   use_label_encoder=False (Deprecated) eval_metric='mlogloss' 을 사용한다\n",
        "model = XGBClassifier(n_estimators=400, \n",
        "                      learning_rate=0.05, \n",
        "                      max_depth=3,\n",
        "                      use_label_encoder=False, \n",
        "                      eval_metric='mlogloss').fit(X_train, y_train)\n",
        "print(f'train {model.score(X_train, y_train):.3f}, test {model.score(X_test, y_test):.3f}')"
      ],
      "id": "JyVDrANFNDeu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ8-KZaBNDeu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "boston = pd.read_csv('boston.csv')\n",
        "X = boston.iloc[:, :-1]\n",
        "y = boston.iloc[:, -1]\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    random_state=0)"
      ],
      "id": "SQ8-KZaBNDeu",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_5IBZDQNDev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "618c54bd-ccf4-49cb-97a5-6a00d9f33f90"
      },
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "# LGBMRegressor를 사용하여 학습 후 성능 출력\n",
        "\n",
        "# n_estimators=100로 사용한다 (여러 가지로 변경해 볼 것, 개수, max_depth)\n",
        "model = LGBMRegressor(n_estimators=100). fit(x_train, y_train)\n",
        "print('train : {:.3f}, test : {:.3f}'.format(model.score(x_train, y_train),\n",
        "                                             model.score(x_test, y_test)))"
      ],
      "id": "C_5IBZDQNDev",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 0.976, test : 0.741\n"
          ]
        }
      ]
    }
  ]
}